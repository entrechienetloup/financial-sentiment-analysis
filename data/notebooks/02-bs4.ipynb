{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T14:43:40.298455Z",
     "start_time": "2025-11-15T14:43:39.507407Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os # Fürs Speichern\n",
    "\n",
    "TARGET_URL = \"https://www.sap.com/investors/de/financial-documents-and-events/recent-results.html\"\n",
    "\n",
    "# WICHTIG: Für SAP immer den ganzen Header aus Netwerkanalyse kopieren, User-Agent alleine geht nicht -> 403 Forbidden\n",
    "HEADERS = {\n",
    "    \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"Accept-Language\": \"de,ja;q=0.5\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    #\"Cookie\": \"optout_domains_pc=; gpcishonored=true; notice_preferences=0:; notice_gdpr_prefs=0::implied,eu; notice_poptime=1726664400000; cmapi_gtm_bl=ga-ms-ua-ta-asp-bzi-sp-awct-cts-csm-img-flc-fls-mpm-mpr-m6d-tc-tdc; cmapi_cookie_privacy=permit 1 required; botinfo_sap=3819c28196071d507e51b1ca2ba9c8ff; country=DE; renderid=rend1; ngds_opt_out=true; TAsessionID=d5306e28-6143-4b67-86bc-ecca2cbb99aa|EXISTING; notice_behavior=implied,eu\",\n",
    "    #braucht nicht unbedingt Cookies\n",
    "    \"DNT\": \"1\",\n",
    "    \"Host\": \"www.sap.com\",\n",
    "    \"Priority\": \"u=0, i\",\n",
    "    \"Referer\": \"https://www.sap.com/investors/de.html\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Sec-GPC\": \"1\",\n",
    "    \"TE\": \"trailers\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:145.0) Gecko/20100101 Firefox/145.0'\n",
    "}\n",
    "\n",
    "\n",
    "# Speicherort\n",
    "PATH_RAW_DATA = \"../raw\" # Pfad angepasst für Notebooks im Root\n",
    "os.makedirs(PATH_RAW_DATA, exist_ok=True)\n",
    "\n",
    "# Liste, in der wir die Daten (als Dictionaries) sammeln\n",
    "collected_data = []\n",
    "\n",
    "\n",
    "# --- 2. Phase: Download (Requests) ---\n",
    "print(f\"Lade HTML von: {TARGET_URL}\")\n",
    "try:\n",
    "    response = requests.get(TARGET_URL, headers=HEADERS)\n",
    "    # Prüfen, ob der Abruf erfolgreich war (Status 200 = OK)\n",
    "    response.raise_for_status()\n",
    "    print(\"Download erfolgreich.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Fehler beim Abruf der URL: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade HTML von: https://www.sap.com/investors/de/financial-documents-and-events/recent-results.html\n",
      "Download erfolgreich.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:48:17.318559Z",
     "start_time": "2025-11-15T14:48:17.293492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --- 3. Phase: Parsing (BeautifulSoup) ---\n",
    "# Erstelle ein \"Suppen\"-Objekt aus dem HTML-Text\n",
    "soup = BeautifulSoup(response.text, 'html.parser') # \"content\" für Bilder usw. geeignet\n",
    "\n",
    "\n",
    "# --- 4. Phase: Extraktion (Das ist Ihre Detektivarbeit!) ---\n",
    "# Hier müssen Sie die Tags und Klassen einsetzen, die Sie in Phase 1 gefunden haben.\n",
    "# BEISPIEL-STRUKTUR (wird bei SAP anders aussehen!)\n",
    "\n",
    "# Suchen wir alle \"News-Boxen\" auf der Seite\n",
    "# Angenommen, jede News ist in einem <article class=\"news-item\">\n",
    "news_items = soup.find_all('div', class_='Grid__row--SkLLf') # Beispiel!\n",
    "\n",
    "if not news_items:\n",
    "    print(\"WARNUNG: Keine 'news-item' gefunden. Prüfen Sie die HTML-Struktur!\")\n",
    "else:\n",
    "    print(f\"{len(news_items)} News-Items gefunden. Verarbeite...\")\n",
    "\n",
    "for item in news_items:\n",
    "    try:\n",
    "        # Finde die Überschrift (z.B. in einem <h2> mit class 'headline')\n",
    "        headline_tag = item.find('h6', class_='Headline__root--lKpRX HeadlineWithLine__headline--YJJNM Headline__headline-xxxxs--gkvoY Headline__medium--irDyM Headline__color--A3sZu')\n",
    "        headline = headline_tag.text.strip() if headline_tag else \"N/A\"\n",
    "\n",
    "        # Finde das Datum (z.B. in einem <span> mit class 'date')\n",
    "        date_tag = item.find('p', class_='Eyebrow__root--lBJPj TextStandard__eyebrow--Kcxze Eyebrow__eyebrow-s--DpVB7')\n",
    "        date = date_tag.text.strip() if date_tag else \"N/A\"\n",
    "\n",
    "        # Finde den Link zum vollen Artikel (im <a>-Tag)\n",
    "        link_tag = item.find('a')\n",
    "        link = link_tag['href'] if link_tag else \"N/A\"\n",
    "\n",
    "        # HIER FOLGT DER \"DEEP SCRAPE\" (siehe unten)\n",
    "        # Fürs Erste speichern wir nur die Metadaten\n",
    "\n",
    "        collected_data.append({\n",
    "            'company': 'SAP', # Hartkodiert, da wir nur SAP scrapen\n",
    "            'date': date,\n",
    "            'headline': headline,\n",
    "            'url': link\n",
    "            # 'full_text' kommt als Nächstes\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Parsen eines Items: {e}\")\n",
    "\n",
    "# --- 5. Phase: Speichern ---\n",
    "df = pd.DataFrame(collected_data)\n",
    "file_path = os.path.join(PATH_RAW_DATA, \"sap_news_metadata.csv\")\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"\\nDaten erfolgreich in {file_path} gespeichert.\")\n",
    "print(df.head())"
   ],
   "id": "681bd439e478c06d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 News-Items gefunden. Verarbeite...\n",
      "\n",
      "Daten erfolgreich in ../raw/sap_news_metadata.csv gespeichert.\n",
      "  company date             headline  \\\n",
      "0     SAP  N/A     22. Oktober 2025   \n",
      "1     SAP  N/A                  N/A   \n",
      "2     SAP  N/A      Auf einen Blick   \n",
      "3     SAP  N/A                  N/A   \n",
      "4     SAP  N/A  Dokumente und Links   \n",
      "\n",
      "                                                 url  \n",
      "0                                                N/A  \n",
      "1                                                N/A  \n",
      "2                                                N/A  \n",
      "3                                                N/A  \n",
      "4  /docs/download/investors/2025/sap-2025-q3-mitt...  \n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
